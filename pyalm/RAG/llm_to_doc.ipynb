{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aed3e7f2-11d5-4090-9fbe-2607b3ffda15",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyalm.models.openai import OpenAI\n",
    "import pyalm\n",
    "from pyalm import ConversationRoles\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import os\n",
    "\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d4b78f5-ba5b-4c15-9df8-0d2ea274430e",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7828c1b2-58dc-49de-961a-ab23eb03df56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document size: 20003 chars\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [01:12<00:00, 36.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished. Took 73s and a total of 8581 tokens to generate 16 entries.\n",
      "Size in chars: 12224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Final step. Look at this when you read the rest.\n",
    "# This will read from a PDF, segment it and try to convert it into a json file that will directly be saved.\n",
    "# The correct metadata set here is up to you.\n",
    "\n",
    "parse_and_store_doc(\"/home/finn/Downloads/PEER_final.pdf\",\n",
    "                    {\"document_title\":\"Lecture Notes on CMB Theory: From Nucleosynthesis to Recombination\",\n",
    "                     \"source_url\":\"https://arxiv.org/abs/0802.3688\" ,\n",
    "                     \"authors\":\"Wayne Hu\",\n",
    "                     \"publisher\":\"arxiv.org\",\n",
    "                     \"tags\":[\"physics\",\"astrophysics\"]},\n",
    "                    read_directly=False\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "243f361c-d3af-4011-9632-67ae74c07513",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_list=None\n",
    "file=None\n",
    "def parse_and_store_doc(filepath, metadata, read_directly=False):\n",
    "    global full_list, file\n",
    "    if read_directly:\n",
    "        # You can use read_directly to e.g read from Latex, a raw text, HTML etc.\n",
    "        with open(filepath, \"r\") as f:\n",
    "            content = f.read()\n",
    "    else:\n",
    "        # This will use tika to read from a PDF, DOCX etc.\n",
    "        # You can also first read from the PDF and then store the content in a variable and use read_directly=True\n",
    "        # Makes sense if you don't just have PDFs but also other formats.\n",
    "        import tika\n",
    "        tika.initVM()\n",
    "        from tika import parser\n",
    "        parsed = parser.from_file(filepath, xmlContent=True)\n",
    "        content = parsed[\"content\"]\n",
    "    #577 tokens for system message\n",
    "    start=time.time()\n",
    "    with open(\"base_tracker.yaml\",\"r\") as f:\n",
    "        base_yaml = f.read()\n",
    "    \n",
    "    doc_title = os.path.basename(filepath).split(\".\")[0]\n",
    "    \n",
    "    metadata = [metadata]\n",
    "    \n",
    "    full_list = metadata\n",
    "    with open(f\"{doc_title}.json\",\"w\") as file:\n",
    "        file.write(json.dumps(full_list))\n",
    "    \n",
    "    plan_size =10000\n",
    "    # If the document is too large, we will split it into chunks of 10000 chars.\n",
    "    # Otherwise we run into forgetfulness issues.\n",
    "    doc_len = len(content)\n",
    "    print(f\"Document size: {doc_len} chars\")\n",
    "    iter = doc_len//plan_size\n",
    "    step_size = int(plan_size+(doc_len%plan_size)/iter)+1\n",
    "    total_tokens = 0\n",
    "    total_chars = 0\n",
    "    for i in tqdm(range(iter)):\n",
    "        llm.conversation_history = pyalm.internal.state.ConversationTracker.from_yaml(base_yaml)\n",
    "        chunk = content[i*step_size:(i+1)*step_size]\n",
    "        llm.add_tracker_entry(chunk,ConversationRoles.USER)\n",
    "        txt = llm.create_completion(max_tokens=3500, chat=True, temperature=0)\n",
    "        total_tokens += llm.finish_meta[\"tokens\"][\"total_tokens\"]\n",
    "        entries = txt.replace(\"```json\", \"\").replace(\"```\", \"\")\n",
    "        entries = entries.replace(\"\\\\\", \"\\\\\\\\\") \n",
    "        try:\n",
    "            total_chars +=len(entries)\n",
    "            entries = json.loads(entries)\n",
    "            full_list += entries\n",
    "            with open(f\"{doc_title}.json\",\"w\") as file:\n",
    "                file.write(json.dumps(full_list))\n",
    "        except Exception as e:\n",
    "            print(f\"Error during turning text into json!\\nEntry {i} will be skipped!\")\n",
    "            print(\"--------\")\n",
    "            print(e)\n",
    "            print(txt)\n",
    "            print(\"--------\")\n",
    "    end = time.time()\n",
    "    file.close()\n",
    "    print(f\"Finished. Took {round(end - start)}s and a total of {total_tokens} tokens to generate {len(full_list)-1} entries.\\nSize in chars: {total_chars}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6879a0e3-ae8e-408f-973c-2f038cdf7b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can use this to directly query the model for completions.\n",
    "# gen = llm.create_generator(\"MESSAGE\",max_tokens=3500, chat=True)\n",
    "# for i in gen:\n",
    "#     print(i[0],end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f7d98285-ab09-4753-bc43-f7eb75a46bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the core of the system. THis tells the model how to actually split the text.\n",
    "# You need to execute this at least once as it creates a \"template\" for the above functions.\n",
    "\n",
    "instruct = \"\"\"You are a bot for content extraction for an embeddings based knowledge retrieval system.\n",
    "A user will prompt you with some form of extracted or otherwise obtained document, usually in the form of html, xml etc.\n",
    "\n",
    "It is your job to transform the entire user input into a json file, fitting for database of \"knowledge snippets\".\n",
    "\n",
    "That means you will transform the text into segments, that each provide meaninguf information.\n",
    "This will be used in the end for knowledge retrieval so each snippet should contain a coherent block of info, but be easily readable.\n",
    "A single sentence or two sentences are not very valuable on their own!\n",
    "\n",
    "It is important that the entirety of the input will be transformed into such segments, so that if one would append all of them together, the document would be restored.\n",
    "\n",
    "You can orient yourself for splitting at headings, however you don't have to.\n",
    "\n",
    "As this will be presented to a human reader the entries need to be cleaned up appropriately. That means that all control sequences from html, xml etc. are removed.\n",
    "And that all math will be turned into latex.\n",
    "\n",
    "It may happen that in the extraction process info may be lost. For example \"A proportion of $3\\frac{4}{2}$ of...\" may be seen by you as \"A proportion of 3 42 of...\".\n",
    "In cases where you see something like this, replace parts with question marks. Incorrect information is far more dangerous as missing information as people will rely on this!\n",
    "\n",
    "For each content entry, try to add infos on where to find the sequence in the original document. This could be the page number, header, subheader etc.\n",
    "Ideally with that a user is able to find the text block quickly in the document.\n",
    "\n",
    "Should the document stop too early to finish a block, do not add it! Only add a coherent block of info!\n",
    "\n",
    "Do not the source section of a document.\n",
    "\n",
    "An ideal output would look like this (this has been shortened to only one entry).\n",
    "\n",
    "[\n",
    "{\n",
    "\"header\":\"Cabibbo–Kobayashi–Maskawa Matrix of Flavor Mixing\",\n",
    "\"subheader\":\"History\"\n",
    "\"page\":1,\n",
    "\"content\":\"By 1950s, physicists have noted that the Fermi constant GF inferred from the $\\beta$–decays of nuclei is a couple of percent smaller than the GF inferred from the muon decay.\n",
    "At the same time, a bunch of strange particles were discovered in cosmic rays and accelerator labs; these particles were created by the strong interactions but decayed only by the weak interactions, hence the name “strange”.\n",
    "Moreover, the effective Fermi constant responsible for the strange particle decays was about $4 \\frac{1}{2}$ times weaker than the regular GF responsible or the nuclear $\\beta$–decays or the pion decays.\"\n",
    "},\n",
    "..\n",
    "]\n",
    "Note especially the length of the content. Do no output entries with less content!\n",
    "\n",
    "Do not respond with anything else but the JSON!\n",
    "Nothing like ```json, no comments, nothing!\n",
    "Your output will be parsed immediately, you would cause parsing errors with this!\n",
    "\n",
    "Do not rewrite any content! You write down content adhering precisely to original formulations.\n",
    "Do not write summaries, interprerations or any of the sort. Your job is to make a parsed document readable and segment it.\n",
    "You do not alter anything!\"\"\"\n",
    "\n",
    "llm.reset_tracker()\n",
    "llm.set_system_message(instruct)\n",
    "\n",
    "with open(\"base_tracker.yaml\", \"w\") as f:\n",
    "    f.write(llm.conversation_history.to_yaml())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda_ml",
   "language": "python",
   "name": "cuda_ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
