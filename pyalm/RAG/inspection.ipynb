{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f7073ff-2eaa-4555-857f-f03e8b1cfb32",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import regex as re\n",
    "import xml.etree.ElementTree as ET"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Usually the hardest part of extracting information from a document is to get the text out of it in a way that is useful.\n",
    "\n",
    "The below snippets give an Ansatz on how to extract information from different types of documents."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a081e3-7c77-47d5-a759-2d7b8e65401c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Extracting text from a PDF\n",
    "\n",
    "This uses tika (java required) to extract the text from a PDF.\n",
    "Usually its smart to check manually what the output is before entering it into the RAG preprocessor.\n",
    "\n",
    "Known problems:\n",
    "- PDFs with double columns\n",
    "- Tables\n",
    "- Formulas (e.g. x^2 often becomes something like x2\n",
    "\"\"\"\n",
    "import tika\n",
    "tika.initVM()\n",
    "from tika import parser\n",
    "parsed = parser.from_file(\"/home/finn/Downloads/PEER_final.pdf\", xmlContent=True)\n",
    "content = parsed[\"content\"]\n",
    "print(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "541b02d5-ba2f-4d8c-bb40-a2fd655f1ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Extracting text from HTML\n",
    "\n",
    "there is no such thing as a general way to extract text from HTML.\n",
    "The below works if there is a clear header based structure in the HTML.\n",
    "With this you can directly get the JSON for input into the RAG database.\n",
    "\n",
    "Usually this wont work and you use the RAG preprocessor and do some filtering before.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def html_to_entities(html_content, source=\"NOT SPECIFIED\"):\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    page_title = soup.title.string if soup.title else \"NONE\"\n",
    "\n",
    "    content_dict = {}\n",
    "    elements = soup.find_all(re.compile(r'h[1-6]|p'))\n",
    "    current_heading = \"\"\n",
    "    for element in elements:\n",
    "        if element.name.startswith('h'):\n",
    "            current_heading = element.get_text().strip()\n",
    "            content_dict[current_heading] = \"\"\n",
    "        elif element.name == 'p' and current_heading:\n",
    "            content_dict[current_heading] += element.get_text().strip() + \" \"\n",
    "    for heading, text in content_dict.items():\n",
    "        content_dict[heading] = ' '.join(text.split())\n",
    "    embeddings = []\n",
    "    for name, item in content_dict.items():\n",
    "        if not item or item == \"\":\n",
    "            continue\n",
    "        entry = {\"document_title\": page_title, \"subtitle\": name, \"content\": item,\n",
    "                 \"source\": source}\n",
    "        embeddings.append(entry)\n",
    "    return embeddings\n",
    "html_to_entities(\"/home/finn/Downloads/hugging.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "050df0d8-51ff-4ca4-86da-ae831f889c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Extracting text from (Wikipedia)XML\n",
    "\n",
    "Working with XML is the easiest.\n",
    "The below directly creates the JSON for the RAG database. No need for the RAG preprocessor.\n",
    "\"\"\"\n",
    "\n",
    "def get_entities_from_wiki_xml(path, tags, doc_id):\n",
    "    \"\"\"\n",
    "    From knowledge_db_creation.py\n",
    "    \"\"\"\n",
    "    tree = ET.parse(path)\n",
    "    root = tree.getroot()\n",
    "    entities = []\n",
    "    for page in root[1:]:\n",
    "\n",
    "        text = page.find(\"{http://www.mediawiki.org/xml/export-0.10/}revision\").find(\n",
    "            \"{http://www.mediawiki.org/xml/export-0.10/}text\").text\n",
    "        title = page.find(\"{http://www.mediawiki.org/xml/export-0.10/}title\").text\n",
    "        id = page.find(\"{http://www.mediawiki.org/xml/export-0.10/}id\").text\n",
    "        if \"Category:\" in title:\n",
    "            continue\n",
    "\n",
    "        def repl(matchobj):\n",
    "            hit = matchobj.groups()[0]\n",
    "            full = matchobj.group()\n",
    "            if \"|\" not in full or \"efn|\" in full:\n",
    "                return \"\"\n",
    "            elif \"math| \" in full:\n",
    "                return f\"${re.sub(r'{{((?:[^{}]|(?R))*)}}', repl, hit[6:])}$\"\n",
    "            elif \"|\" in hit:\n",
    "                hit = re.sub(r\"\\|link=y\", r\"\", full)\n",
    "                if \"10^|\" in hit:\n",
    "                    return f\"10^{hit[6:-2]}\"\n",
    "                hit = re.sub(r\"{{(.*?)\\|(.*?)}}\", r\"\\2\", hit)\n",
    "                return hit\n",
    "            else:\n",
    "                return full\n",
    "\n",
    "        sections = re.split(r'={2,5}\\s*(.*?)\\s*={2,5}', text)\n",
    "        headers = [title] + sections[1::2]\n",
    "        section_text = sections[0::2]\n",
    "        sections = {i: j for i, j in zip(headers, section_text)}\n",
    "        entries_to_remove = (\n",
    "            'See also', 'Footnotes', \"References\", \"Sources\", \"History\", \"External links\", \"Bibliography\")\n",
    "        for k in entries_to_remove:\n",
    "            sections.pop(k, None)\n",
    "\n",
    "        for i in sections:\n",
    "            text = sections[i]\n",
    "            text = text.replace(\"&lt;\", \"<\")\n",
    "            text = text.replace(\"&gt;\", \">\")\n",
    "            text = re.sub(r'\\[\\[(.*?)(?:\\|.*?)?\\]\\]', r'\\1', text)\n",
    "            text = re.sub(r\"<ref (.*?)>(.*?)</ref>\", '', text)\n",
    "            text = re.sub(r\"<ref>(.*?)</ref>\", '', text)\n",
    "            text = re.sub(r\"<ref (.*?)>\", '', text)\n",
    "            text = re.sub(r\"<math(.*?)>(.*?)</math>\", r'$\\2$', text)\n",
    "            text = re.sub(r\"<sub>(.*?)</sub>\", r'$\\1$', text)\n",
    "            text = re.sub(r\"<sup>(.*?)</sup>\", r'^{\\1}', text)\n",
    "            text = re.sub(\"&nbsp;\", \" \", text)\n",
    "            text = re.sub(\"\\t;\", \"\", text)\n",
    "            text = re.sub(r\" {2,20}\", \"\", text)\n",
    "            text = re.sub(r'{{((?:[^{}]|(?R))*)}}', repl, text)\n",
    "            text = re.sub(\"\\n\", \"\", text)  # <ref></ref>\n",
    "            text = re.sub(r\"<ref>(.*?)</ref>\", '', text)\n",
    "            text = re.sub(r\"\\'\\'\\'(.*?)\\'\\'\\'\", r\"'\\1'\", text)\n",
    "            text = re.sub(r\"\\'\\'(.*?)\\'\\'\", r\"'\\1'\", text)\n",
    "            entity = {\"header\": title, \"content\": i + \":\\n\" + text,\n",
    "                      \"url\": f\"https://en.wikipedia.org/?curid={id}#\" + \"_\".join(i.split(\" \")),\n",
    "                      \"subheader\": i, \"tags\":tags, \"doc_id\": doc_id}\n",
    "            entities.append(entity)\n",
    "            # sections[i] = text\n",
    "    return entities"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
